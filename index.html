<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <title>XAIP-Workshops by KCL-Planning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
      
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">XAIP Workshops</h1>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/" class="btn">Home</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/related_work" class="btn">Related Work</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/contact" class="btn">Contact</a>
      <a href="https://github.com/KCL-Planning/XAIP-Workshops/" class="btn">View on GitHub</a>

    </section>

    <section class="main-content">
      <p>Explainable Artificial Intelligence (XAI) concerns the challenge of shedding light on opaque models where transparency is important, e.g. in analysis or synthesis tasks. In particular, as AI is increasingly being adopted for deployed applications, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support collaborations with humans but also it is a necessary step in the process of building trust as humans migrate greater competence and responsibility to AI systems. The challenge is to find effective ways to characterize and communicate the foundations of AI-driven behavior when the algorithms that drive it are far from transparent to humans. While XAI at large is primarily concerned with learning-based approaches, model-based <strong>Explainable AI Planning (XAIP)</strong> can play an important role in addressing complex decision-making procedures.</p>
      <p></p>
      <h3>ICAPS Workshops</h3>
      <p></p>
     
      <ul><li>
      <b>2019 ICAPS XAIP Workshop</b>
      <br>See the workshop <a href="https://icaps19.icaps-conference.org/workshops/XAIP/index.html">webpage</a> for more information.
      </li></ul>
      
      <ul><li>
      <b>2018 ICAPS XAIP Workshop</b>
        <ul>
          <li><b>Mission:</b><br><font size="-1"> As AI is increasingly being adopted into application solutions, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support integrated working styles, in which humans and intelligent systems cooperate in problem-solving, but it is also a necessary step in the process of building trust as humans invest greater authority and responsibility in intelligent systems. Explainability poses challenges for many types of AI systems, including planning and scheduling (PS) systems. For example, how should a PS system justify that a plan or schedule is correct, or good, or respects supplied preferences? How can the PS system explain particular steps, ordering decisions, or resource choices? How can a PS system explain that no solution is possible, or what relaxations of the constraints would allow a solution? How can a PS system respond to questions like “what is the hard part?” or “why is this taking so long?”. These are all difficult questions that can require analysis of plan or schedule structure, analysis of the goals, constraints, and preferences, and potentially hypothetical reasoning.</font></li>
          <li><b>Topics</b> (including but not limited to):<br><ul>
            <li>representation, organization, and knowledge needed for explanation;</li>
            <li>the creation of such content during plan generation and understanding;</li>
            <li>generation and evaluation of explanations;</li>
            <li>the way in which explanations are communicated to humans (e.g., plan summaries, answers to questions);</li>
            <li>the role of knowledge and learning in explainable planners;</li>
            <li>human vs AI models in explanations;</li>
            <li>links between explainable planning and other disciplines (e.g., social science, argumentation);</li>
            <li>use cases and applications of explainable planning</li>
            </ul></li>
          <li><b>Important Dates:</b><br><ul>
            <li>Paper submission: April 6, 2018</li>
            <li>Notification: April 26, 2018</li>
            <li>Camera-ready submission: May 25, 2018</li>
            <li>Date of Workshop: June 25, 2018</li>
            </ul></li>
          <li><b>Invited Speaker:</b><br><b>David Aha</b>, Naval Research Laboratory</li>
          <li><b>Program</b><br></li>
          <li><b>Proceedings:</b><br></li>
          <li><b>Organizing Chairs/Program Committee</b><br></li>
        </ul>
      <br>See the workshop <a href="http://icaps18.icaps-conference.org/xaip/index.html">webpage</a> for more information.
      </li></ul>
      
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/KCL-Planning/XAIP-Workshops">XAIP Workshops</a> is maintained by <a href="https://github.com/KCL-Planning">KCL-Planning</a>.</span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>
    </section>
  </body>
</html>
