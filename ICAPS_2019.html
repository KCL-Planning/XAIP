<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <title>XAIP-Workshops by KCL-Planning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
      
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">XAIP Workshops</h1>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/index.html" class="btn">Home</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/ICAPS_2019" class="btn">XAIP 2019</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/archive" class="btn">Archive</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/related_work" class="btn">Related Work</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/contact" class="btn">Contact</a>
   
    </section>

    <section class="main-content">
     
     <center>
      <h1 id="icaps19"><b>2nd ICAPS Workshop on Explainable Planning (XAIP-2019)</b></h1> <h3>Berkeley, CA, 12 July 2019.</h3>
    </center>       

<h3><b><i>Please see submitted papers and provide feedback <a href="https://openreview.net/group?id=icaps-conference.org/ICAPS/2019/Workshop/XAIP" target="_blank">through OpenReview.net</a></i></b></h3>

          <h2><b>Mission:</b></h2><p>Explainable Artificial Intelligence (XAI) concerns the challenge of shedding light on opaque models in contexts for which transparency is important, i.e. where these models could be used to solve analysis or synthesis tasks. In particular, as AI is increasingly being adopted into application solutions, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support integrated working styles, in which humans and intelligent systems cooperate in problem-solving, but also it is a necessary step in the process of building trust as humans migrate greater competence and responsibility to such systems. The challenge is to find effective ways to characterise, and to communicate, the foundations of AI-driven behaviour, when the algorithms that drive it are far from transparent to humans. While XAI at large is primarily concerned with learning-based approaches, model-based approaches are well suited -- arguably better suited -- for explanation, and Explainable AI Planning (XAIP) can play an important role in addressing complex decision-making procedures. <br><br>
            After the success of previous workshops on XAI and XAIP, the mission of this workshop is to mature and broaden the XAIP community, fostering continued exchange on XAIP topics at ICAPS.</p></li>
          <h2><b>Topics</b></h2> <p>
            Topics of interest include (but are not limited to):
            <ul>
            <li>Frameworks for defining meaningful explanations in planning and scheduling contexts;</li>
            <li>Representation, organization, and memory content used in explanation;</li>
            <li>The creation of such content during plan generation or understanding;</li>
            <li>Generation and evaluation of explanations;</li>
            <li>The explanation process, i.e. the way in which explanations are communicated to humans (e.g., plan summaries, answers to questions);</li>
            <li>The role of knowledge and learning in explainable planners;</li>
            <li>Human vs AI models in explanations;</li>
            <li>Links between explainable planning and other disciplines (e.g. social science, argumentation);</li>
            <li>Model differences and model reconciliation;</li>
            <li>Goal reasoning and plan explanations;</li>
            <li>Excuse generation, unsolvability and explanations;</li>
            <li>Use cases and applications of explainable planning.</li>
            </ul></p></li>
<h2><b>Submissions:</b></h2><p>
Given the novelty of XAIP as an area, we invite several different kinds of submissions:
<ul>
<li><i>full technical papers</i>, making an original contribution; up to 9 pages including references;</li>
<li><i>short technical papers</i>, making an original contribution; up to 5 pages including references;</li>
  <li>
<i>position papers</i>, proposing XAIP challenges, outlining XAIP ideas, debating issues relevant to XAIP; up to 5 pages including references.</li>
</ul>

<br>

Please format submissions in AAAI style (see instructions in the Author Kit at <a href="http://www.aaai.org/Publications/Templates/AuthorKit19.zip">http://www.aaai.org/Publications/Templates/AuthorKit19.zip</a>). Authors considering submitting to the workshop papers rejected from the main conference, please ensure you do your utmost to address the comments given by ICAPS reviewers. Please do not submit papers that are already accepted for the main conference to the workshop. 
<br><br>
Every submission will be reviewed by members of the program committee according to the usual criteria such as relevance to the workshop, significance of the contribution, and technical quality. 
Authors can select if they want their submissions to be single-blind or double-blind (recommended for IJCAI dual submissions) at the time of submission. The OpenReview forum will also allow the public to comment and provide feedback on the papers (but authors will remain anonymous to the public). 
<br>

Submit your papers here: <a href="https://openreview.net/group?id=icaps-conference.org/ICAPS/2019/Workshop/XIAP" target="_blank">https://openreview.net/group?id=icaps-conference.org/ICAPS/2019/Workshop/XIAP</a> 
<br><br>
Submissions sent to other conferences are allowed if this does not interfere with their submission rules. Submissions under double-blind review in another conference (in particular IJCAI-19) must be anonymous. 
<br><br>
The workshop is meant to be an open and inclusive forum, and we encourage papers that report on work in progress or that do not fit the mold of a typical conference paper. 
<br><br>
At least one author of each accepted paper must attend the workshop in order to present the paper. Authors must register for the ICAPS main conference in order to attend the workshop. There will be no separate workshop-only registration.
          

          <h2><b>Important Dates:</b></h2><p>
            <li>Paper submission: April 15, 2019 (UTC-12)</li>
            <li>Notification: May 15, 2019</li>
            <li>ICAPS early registration: May 17, 2019</li>
            <li>Camera-ready submission: TBD</li>
            <li>Date of Workshop: July 12th, 2019</li>
            </ul></p></li>

          <h2><b>Invited Speaker:</b></h2><p>
          	<b>Robert R. Hoffman</b>, Institute for Human and Machine Cognition<br><br>
          	<i><b>Macrocognition: Foundations for Planning and Explanation</b><br>
          	Macrocognition is how cognition adapts to complexity. The historical roots of macrocognition reach back to the late 1800s, and the essentials of the paradigm have been fairly well specified. <br>

The models of sensemaking, flexecution, coordination, re-learning, and mental projection help clarify differences between macrocognitive and microcognitive approaches. Microcognitive models are based on causal chains having distinct start and stop (or input-output) points. On the other hand, macrocognitive models are cyclical and closed-loop. Microcognitive models are useful in hindsight, to tell stories; macrocognitive models are transcendent and anticipatory.  
<br>
The primary macrocognitive functions correspond with the "Families of Laws of Complex Cognitive Systems" developed by David Woods. The Families are based on five fundamental bounds on complex human-machine work systems. 
<br>
Noteworthy aspects of macrocognition are pertinent to planning systems technology:
<ul>
<li>	Flexecution emphasizes the fact that goals morph even as they are being pursued.
<li>	Re-grounding embraces the fact that planning and plan execution are team activities.
<li>	Projection elaborates on how planning is anticipatory. 
</ul>
These macrocognitive concepts and models have implications for Explainable AI (XAI) systems. If we present to a user an AI planning system that explains how it works, how do we know whether the explanation works and the user has made sense of the AI and is able to flexecute with it? In other words, how do we know that an XAI system is any good? Key concepts of measurement include specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how well the human-XAI work system performs.</i> 



      
      </p></li>

          <h2><b>Program:</b></h2><p>
    <table style="width:100%">
  <tr>
    <th align="center">8:30-8:45</th>
    <th>Introduction</th>
  </tr> 
  <tr>
    <th>8:45-9:45</th>
    <th>Invited Talk by Robert Hoffman<br><i>Macrocognition: Foundations for Planning and Explanation</i> </th>
  </tr> 

  <tr>
    <th>9:45 - 10:30</th>
    <th>Session 1</th>
  </tr> 
  <tr>
    <td></td>
    <td align="left">
<i><b>Design for Interpretability</b></i><br>
Anagha Kulkarni, Sarath Sreedharan, Sarah Keren, Tathagata Chakraborti, Subbarao Kambhampati<br>
<i><b>Explainable Planning as a Service</b></i><br>
Michael Cashmore, Anna Collins, Benjamin Krarup, Senka Krivic, Daniele Magazzeni, David Smith<br>
<i><b>Varieties of Explainable Agency</b></i><br>
Pat Langley<br>
<i><b>Challenges of Explaining Real-Time Planning</b></i><br>
Adrian Agogino, Ritchie Lee, Dimitra Giannakopoulou<br>
    </td>
  </tr> 
<tr>
    <th>10.30 - 11:00</th>
    <th>BREAK </th>
  </tr> 
  <tr>
    <th>11:00 - 12:00</th>
    <th>Session 2</th>
  </tr> 
  <tr>
    <td></td>
    <td align="left">
<i><b>Combining Cognitive and Affective Measures with Epistemic Planning for Explanation Generation</b></i><br>
Ronald P. A. Petrick, Sara Dalzel-Job, Robin L. Hill<br>
<i><b>Feature-directed Active Learning for Learning User Preferences</b></i><br>
Sriram Gopalakrishnan, Utkarsh Soni, Subbarao Kambhampati<br>
<i><b>Online Explanation Generation for Human-Robot Teaming</b></i><br>
Mehrdad Zakershahrak, Ze Gong, Akkamahadevi Hanni, Yu Zhang<br>
<i><b>Model-Free Model Reconciliation</b></i><br>
Sarath Sreedharan, Alberto Olmo, Aditya Prasad Mishra, Subbarao Kambhampa<br>
<i><b>Human-Understandable Explanations of Infeasibility for Resource-Constrained Scheduling Problems</b></i><br>
Niklas Lauffer, Ufuk Topcu<br>
<i><b>A General Framework for Synthesizing and Executing Self-Explaining Plans for Human-AI Interaction</b></i><br>
Sarath Sreedharan, Tathagata Chakraborti, Christian Muise, Subbarao Kambhampati<br>
    </td>
  </tr> 

<tr>
    <th>12:00 - 12:30</th>
    <th>Posters for sessions 1 and 2</th>
  </tr> 

  <tr>
    <th>12:30 - 14:00</th>
    <th>LUNCH</th>
  </tr> 

  <tr>
    <th>14:00 - 15:00</th>
    <th>Session 3</th>
  </tr> 
  <tr>
    <td></td>
    <td align="left">
<i><b>Model-Based Contrastive Explanations for Explainable Planning</b></i><br>
Benjamin Krarup, Michael Cashmore, Daniele Magazzeni, Tim Miller<br>
<i><b>Explaining the Space of Plans through Plan-Property Dependencies</b></i><br>
Rebecca Eifler, Michael Cashmore, Jörg Hoffmann, Daniele Magazzeni, Marcel Steinmetz<br>
<i><b>A General Logic-based Approach for Explanation Generation</b></i><br>
Stylianos Loukas Vasileiou, William Yeoh, Tran Cao Son<br>
<i><b>Bayesian Inference of Temporal Specifications to Explain How Plans Differ</b></i><br>
Joseph Kim, Christian Muise, Ankit Shah, Shubham Agarwal, Julie Shah<br>
<i><b>Why Couldn't You do that? Explaining Unsolvability of Classical Planning Problems in the Presence of Plan Advice</b></i><br>
Sarath Sreedharan, Siddharth Srivastava, David Smith, Subbarao Kambhampati<br>
<i><b>Towards an argumentation-based approach to explainable planning</b></i><br>
Anna Collins, Daniele Magazzeni, Simon Parsons<br>
    </td>
  </tr>
<tr>
    <th>15:00 - 15:30</th>
    <th>Posters for session 3</th>
  </tr> 

<tr>
    <th>15.30 - 16:00</th>
    <th>BREAK </th>
  </tr> 


  <tr>
    <th>16:00 - 17:00</th>
    <th>Session 4</th>
  </tr> 


  <tr>
    <td></td>
    <td align="left">
<i><b>When Agents Talk Back: Rebellious Explanations</b></i><br>
Ben Wright, Mark Roberts, David W. Aha, Ben Brumback<br>
<i><b>(How) Can AI Bots Lie?</b></i><br>
Tathagata Chakraborti, Subbarao Kambhampati<br>
<i><b>Domain-independent Plan Intervention When Users Unwittingly Facilitate Attacks</b></i><br>
Sachini Weerawardhana, Darrell Whitley, Mark Roberts<br>
<i><b>Robust Goal Recognition with Operator-Counting Heuristics</b></i><br>
Felipe Meneguzzi, André Grahl Pereira, Ramon F. Pereira<br>
<i><b>Branching-Bounded Contingent Planning via Belief Space Search</b></i><br>
Kevin McAreavey, Kim Bauters, Weiru Liu, Jun Hong<br>
    </td>
  </tr>
  <tr>
    <th>17:00 - 17:30</th>
    <th>Posters for session 4</th>
  </tr> 


</table> 
            </p></li>


          <h2><b>Accepted Papers:</b></h2><p>
<ul>
<li><i>            Human-Understandable Explanations of Infeasibility for Resource-Constrained Scheduling Problems</i>  
<br>Niklas Lauffer, Ufuk Topcu
 
<li><i>Online Explanation Generation for Human-Robot Teaming</i>  
<br>Mehrdad Zakershahrak, Ze Gong, Akkamahadevi Hanni, Yu Zhang
 
<li><i>Model-Based Contrastive Explanations for Explainable Planning</i>  
<br>Benjamin Krarup, Michael Cashmore, Daniele Magazzeni, Tim Miller
 
<li><i>Combining Cognitive and Affective Measures with Epistemic Planning for Explanation Generation</i>  
<br>Ronald P. A. Petrick, Sara Dalzel-Job, Robin L. Hill
 
<li><i>Challenges of Explaining Real-Time Planning</i>  
<br>Adrian Agogino, Ritchie Lee, Dimitra Giannakopoulou
 
<li><i>Design for Interpretability</i>  
<br>Anagha Kulkarni, Sarath Sreedharan, Sarah Keren, Tathagata Chakraborti, Subbarao Kambhampati
 
<li><i>Explainable Planning as a Service</i>  
<br>Michael Cashmore, Anna Collins, Benjamin Krarup, Senka Krivic, Daniele Magazzeni, David Smith
 
<li><i>(How) Can AI Bots Lie?</i>  
<br>Tathagata Chakraborti, Subbarao Kambhampati
 
<li><i>Varieties of Explainable Agency</i>  
<br>Pat Langley
 
<li><i>Towards an argumentation-based approach to explainable planning</i>  
<br>Anna Collins, Daniele Magazzeni, Simon Parsons
 
<li><i>Feature-directed Active Learning for Learning User Preferences</i>  
<br>Sriram Gopalakrishnan, Utkarsh Soni, Subbarao Kambhampati
 
<li><i>Model-Free Model Reconciliation</i>  
<br>Sarath Sreedharan, Alberto Olmo, Aditya Prasad Mishra, Subbarao Kambhampati
 
<li><i>A General Framework for Synthesizing and Executing Self-Explaining Plans for Human-AI Interaction</i>  
<br>Sarath Sreedharan, Tathagata Chakraborti, Christian Muise, Subbarao Kambhampati
 
<li><i>Why Couldn't You do that? Explaining Unsolvability of Classical Planning Problems in the Presence of Plan Advice</i>  
<br>Sarath Sreedharan, Siddharth Srivastava, David Smith, Subbarao Kambhampati
 
<li><i>Bayesian Inference of Temporal Specifications to Explain How Plans Differ</i>  
<br>Joseph Kim, Christian Muise, Ankit Shah, Shubham Agarwal, Julie Shah
 
<li><i>A General Logic-based Approach for Explanation Generation</i>  
<br>Stylianos Loukas Vasileiou, William Yeoh, Tran Cao Son
 
<li><i>When Agents Talk Back: Rebellious Explanations</i>  
<br>Ben Wright, Mark Roberts, David W. Aha, Ben Brumback
 
<li><i>Branching-Bounded Contingent Planning via Belief Space Search</i>  
<br>Kevin McAreavey, Kim Bauters, Weiru Liu, Jun Hong
 
<li><i>Domain-independent Plan Intervention When Users Unwittingly Facilitate Attacks</i>  
<br>Sachini Weerawardhana, Darrell Whitley, Mark Roberts
 
<li><i>Robust Goal Recognition with Operator-Counting Heuristics</i>  
<br>Felipe Meneguzzi, André Grahl Pereira, Ramon F. Pereira
 
<li><i>Explaining the Space of Plans through Plan-Property Dependencies</i>  
<br>Rebecca Eifler, Michael Cashmore, Jörg Hoffmann, Daniele Magazzeni, Marcel Steinmetz
 
 </ul>

<h2><b>Sister Workshops:</b></h2><p>
            Please consider our sister workshops at AAMAS and IJCAI!

<br><b>EXTRAAMAS</b>: <a href="https://extraamas.ehealth.hevs.ch/">visit website</a>
<br><b>XAI</b>: <a href="https://sites.google.com/view/xai2019/home">visit website</a>

            </b> 
            </p></li>




          <h2><b>Organizing Chairs:</b></h2><p><ul>
            <li>Tathagata Chakraborti (IBM Research AI, USA)</li>
            <li>Dustin Dannenhauer (Navatek, LLC, USA)</li>
            <li>Joerg Hoffmann (Saarland University, Germany)</li>
            <li>Daniele Magazzeni (King's College London, UK)</li>
            </ul></p></li>
          <h2><b>Program Committee:</b></h2>
        <ul>
<li>David Aha (Naval Research Laboratory) 
<li>Susanne Biundo (University of Ulm)
<li>John Bresina (NASA) 
<li>Michael Cashmore (King's College London) 
<li>Tathagata Chakraborti (Arizona State University)
<li>Dustin Dannenhauer (Navatek, LLC)
<li>Jeremy Frank (NASA) 
<li>Joerg Hoffmann (Saarland University) 
<li>Senka Krivic (King's College London)  
<li>Pat Langley (University of Auckland) 
<li>Daniele Magazzeni (King’s College London) 
<li>Fabio Mercorio (University of Milan-Bicocca)
<li>Tim Miller (The University of Melbourne) 
<li>Matthew Molineaux (Wright State Research Institute) 
<li>Mark Roberts (Naval Research Laboratory)
<li>David Smith 
<li>Siddharth Srivastava (ASU)
<li>Kartik Talamadupula (IBM) 
<li>Ben Wright (Naval Research Laboratory) 
</ul>


   
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/KCL-Planning/XAIP-Workshops">XAIP Workshops</a> is maintained by <a href="https://github.com/KCL-Planning">KCL-Planning</a>.</span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>
    </section>
  </body>
</html>
